Algorithms for supervised learning of penalty functions for change detection

[[https://travis-ci.org/tdhock/penaltyLearning][https://travis-ci.org/tdhock/penaltyLearning.png?branch=master]]

** Installation

#+BEGIN_SRC R
devtools::install_github("tdhock/penaltyLearning")
#+END_SRC

** Usage

This R package provides a practical implementation of the algorithms
described in our ICML'13 paper, [[http://jmlr.org/proceedings/papers/v28/hocking13.html][Learning Sparse Penalties for
Change-point Detection using Max Margin Interval Regression]]. This
package can be used to learn a penalty function for predicting
change-points in labeled data sets such as [[https://cran.r-project.org/package=neuroblastoma][neuroblastoma]]. Below we
plot the data and labels, for a subset of 6 profiles.

#+BEGIN_SRC R
  library(penaltyLearning)
  ids.str <- paste(c(1, 4, 6, 8, 10, 11))
  someProfiles <- function(all.profiles){
    data.table(all.profiles)[profile.id %in% ids.str, ]
  }
  data(neuroblastoma, package="neuroblastoma")
  profiles <- someProfiles(neuroblastoma$profiles)
  labels <- someProfiles(neuroblastoma$annotations)
  ## Plot labels along with noisy data sets.
  breakpoint.colors <- c(
    "breakpoint"="#a445ee",
    "normal"="#f6f4bf")
  library(ggplot2)
  ggplot()+
    ggtitle("supervised change-point detection = data + labels")+
    theme_bw()+
    theme(panel.margin=grid::unit(0, "lines"))+
    facet_grid(profile.id ~ chromosome, scales="free", space="free_x")+
    geom_tallrect(aes(xmin=min/1e6, xmax=max/1e6, fill=annotation),
                  color="grey",
                  data=labels)+
    scale_fill_manual("label", values=breakpoint.colors)+
    geom_point(aes(position/1e6, logratio),
               data=profiles,
               shape=1)+
    scale_x_continuous(
      "position on chromosome (mega bases)",
      breaks=c(100, 200))
#+END_SRC

The plot above shows 24 * 6 = 144 separate multiple change-point
detection problems, which include labels (colored rectangles) which
indicate regions that should have at least one change (purple
breakpoint labels), and regions that should have no changes (yellow
normal labels). These are positive and negative labels that can be
used to train a supervised machine learning model for change-point
detection.

The change-point detection model that has proven to be most accurate
in these data ([[http://members.cbio.mines-paristech.fr/~thocking/neuroblastoma/accuracy.html][see benchmark]]) is maximum likelihood segmentation with
the normal/Gaussian loss. This model has efficient implementations in
R as =cghseg:::segmeanCO=, =Segmentor3IsBack::Segmentor=, and
=changepoint::cpt.mean=. These packages provide methods for computing
the most likely change-point positions, subject to a constraint on the
number of change-points. In the code below, we use Segmentor to
compute models from 1 to 10 segments.

#+BEGIN_SRC R
  problem.list <- split(profiles, profiles[, paste(profile.id, chromosome)])
  segs.list <- list()
  loss.list <- list()
  for(problem.i in seq_along(problem.list)){
    problem.name <- names(problem.list)[[problem.i]]
    cat(sprintf(
      "%4d / %4d problems %s\n",
      problem.i, length(problem.list), problem.name))
    pro <- problem.list[[problem.name]]
    meta <- pro[1, .(profile.id, chromosome)]
    max.segments <- min(nrow(pro), 10)
    fit <- Segmentor3IsBack::Segmentor(
      pro$logratio, model=2, Kmax=max.segments)
    for(n.segments in 1:max.segments){
      end <- fit@breaks[n.segments, 1:n.segments]
      data.before.change <- end[-n.segments]
      data.after.change <- data.before.change+1
      pos.before.change <- as.integer(
	(pro$position[data.before.change]+pro$position[data.after.change])/2)
      start <- c(1, data.after.change)
      chromStart <- c(pro$position[1], pos.before.change)
      chromEnd <- c(pos.before.change, max(pro$position))
      seg.mean.vec <- fit@parameters[n.segments, 1:n.segments]
      segs.list[[paste(problem.name, n.segments)]] <- data.table(
	meta,
	n.segments,
	start,
	end,
	chromStart,
	chromEnd,
	mean=seg.mean.vec)
    }
    loss.list[[paste(problem.name, n.segments)]] <- data.table(
      meta,
      n.segments=1:max.segments,
      loss=as.numeric(fit@likelihood))
  }
  loss <- do.call(rbind, loss.list)
  segs <- do.call(rbind, segs.list)
#+END_SRC

Choosing the number of change-points is typically done using a penalty
function, which affects the error rate of the change-point model. The
=penaltyLearning= package can be used to measure the error rate by
counting the number of incorrectly predicted labels, and learn a
penalty function which minimizes the error rate. First, we use
=modelSelection= to determine a mapping between penalty values and
segments/changes.

#+BEGIN_SRC R
  selection <- loss[, {
    penaltyLearning::modelSelection(.SD, "loss", "n.segments")
  }, by=.(profile.id, chromosome)]
#+END_SRC

Then, we compute the number of incorrect labels for each model, for
each labeled segmentation problem.

#+BEGIN_SRC R
  changes <- segs[1 < start, ]
  errors <- penaltyLearning::labelError(
    selection, labels, changes,
    change.var="chromStart",
    label.vars=c("min", "max"),
    problem.vars=c("profile.id", "chromosome"))
#+END_SRC

Now, let's perform a computational cross-validation experiment to
train and evaluate a learned penalty function. We use labels in
chromosome 11 as a test set, and use all other labels as a train set.

#+BEGIN_SRC R
  all.errors <- data.table(errors$model.errors)
  all.errors[, set := ifelse(chromosome=="11", "test", "train")]
#+END_SRC

To train a penalty learning model, we compute target intervals of
log(penalty) values that achieve the minimal number of incorrect
labels (for each problem independently). This is the "output" in the
machine learning problem.

#+BEGIN_SRC R
  target.dt <- targetIntervals(
    all.errors[set=="train", ],
    c("profile.id", "chromosome"))
  target.mat <- target.dt[, cbind(min.log.lambda, max.log.lambda)]
  rownames(target.mat) <- target.dt[, paste(profile.id, chromosome)]
#+END_SRC

Then we compute a feature matrix (problems x features), which is the
"input" in the machine learning problem.

#+BEGIN_SRC R
  feature.dt <- profiles[, list(
    log.data=log(.N),
    log.var=log(median(abs(diff(logratio))))
  ), by=.(profile.id, chromosome)]
  all.feature.mat <- feature.dt[, cbind(log.data, log.var)]
  rownames(all.feature.mat) <- feature.dt[, paste(profile.id, chromosome)]
  train.feature.mat <- all.feature.mat[rownames(target.mat), ]
#+END_SRC

Then we use =IntervalRegressionUnregularized= to learn a penalty
function for this small data set (see our [[http://jmlr.org/proceedings/papers/v28/hocking13.html][ICML'13 paper]] for
details). For data sets with more labels and features,
=IntervalRegressionCV= would be preferable, since it also performs
variable selection using L1-regularization.

#+BEGIN_SRC R
  fit <- IntervalRegressionUnregularized(train.feature.mat, target.mat)
#+END_SRC

Then we use the model to predict log(penalty) values for each
segmentation problem (even those in the test set).

#+BEGIN_SRC R
  feature.dt[, pred.log.lambda := predict(fit, all.feature.mat)]
#+END_SRC

We can use =ROChange= to compute test ROC curves, and Area Under the
Curve (AUC), which in this case is 1, indicating perfect prediction
accuracy.

#+BEGIN_SRC R
  test.pred <- feature.dt[chromosome=="11",]
  ROChange(all.errors, test.pred, c("profile.id", "chromosome"))
#+END_SRC

Finally we use a non-equi join of =feature.dt= (which contains the
predicted penalty values) with =selection= (which contains the number
of segments for each penalty value). Then we plot the predicted
models. It is clear that in these data, all of the predicted models
are consistent with the labels.

#+BEGIN_SRC R
  pred.models <- feature.dt[selection, nomatch=0L, on=list(
    profile.id, chromosome,
    pred.log.lambda < max.log.lambda,
    pred.log.lambda > min.log.lambda)]
  pred.segs <- segs[pred.models, on=list(profile.id, chromosome, n.segments)]
  pred.changes <- pred.segs[1 < start, ]
  pred.labels <- errors$label.errors[pred.models, nomatch=0L, on=list(
    profile.id, chromosome, n.segments)]
  ggplot()+
    ggtitle("data + labels + predicted segment means and changes")+
    theme_bw()+
    theme(panel.margin=grid::unit(0, "lines"))+
    facet_grid(profile.id ~ chromosome, scales="free", space="free_x")+
    geom_tallrect(aes(
      xmin=min/1e6, xmax=max/1e6, fill=annotation, linetype=status),
      size=1.5,
      data=pred.labels)+
    scale_linetype_manual("error type",
                          values=c(correct=0,
                            "false negative"=3,
                            "false positive"=1))+
    scale_fill_manual("label", values=breakpoint.colors)+
    geom_point(aes(position/1e6, logratio),
               data=profiles,
               shape=1)+
    scale_x_continuous(
      "position on chromosome (mega bases)",
      breaks=c(100, 200))+
    geom_segment(aes(chromStart/1e6, mean, xend=chromEnd/1e6, yend=mean),
		 data=pred.segs,
		 color="green")+
    geom_vline(aes(xintercept=chromStart/1e6),
               data=pred.changes,
               color="green",
               linetype="dashed")
#+END_SRC

** For more info
See https://github.com/tdhock/change-tutorial
